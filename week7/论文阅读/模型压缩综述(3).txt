Model compression as constrained optimization,
with application to neural nets.
Part I general framework.

⼤型神经网络模型已成为解决各种机器学习和人工智能问题的最新实际解决⽅案的核⼼组成部分。但为了方便部署和使用，压缩模型并保持该有的效果，成为了一个热门的问题。

第一种想法则是，保持原有的模型结构，通过降低模型参数的维度，将参数进行保存，再通过运行时复原从而得到我们想要的分析结果。
第二种则是，改变原有结构，其中参数调整则包含了很多方法，包括：通过loss值直接学习，直接压缩成低秩参数，模型压缩，蒸馏等等。可以将压缩问题从数学的角度视为一个约束最优化问题。

压缩的类型包括，低秩压缩（将权重矩阵分解为较小的矩阵）量化压缩，低精度压缩，剪枝（只包含部分非零值，减少运算），无损压缩等。他们可以相互结合。
好的压缩方法应该有较少的错误，并且应该压缩和解压缩的过程并不复杂。

部分关于模型压缩的表达公式，都是在约束条件下进行最优解的求解，可以通过惩罚函数等方式，精心设计。


