本次实验是多项选择任务，该任务类似于问答，区别在于是提供几个候选答案以及上下文，并训练模型以选择正确答案。
本次选择的模型，是BERT，数据集则是SWAG。swag数据集就是提出了多个选项，然后在给出的句子中选择下一句。词向量化我们还是选择google-bert对应的词向量化，但由于没有直接对应多种选项的api我们需要自行处理，即我们将句子复制多份，然后分别选择给出的选项进行拼接，再贴上是否正确的标签即可。
Transformers没有用于多选的数据整理器，因此需要调整DataCollatorWithPadding来创建batch，通过填充等形成适当的批处理数据。
接下里则是设计超参数，然后将数据以及迭代全部放入即可进行训练。
bert模型较大，训练时间较长，经历三轮微调模型效果确实可以见到有提升，最后设计测试案例（难度不大的案例），改变顺序多次测试，均可返回正确的标签。