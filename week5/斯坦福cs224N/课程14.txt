大型语言模型：训练过程中第一部分的无监督学习对下游任务有较大的贡献。
T5是将文本转换为文本作为基本核心，可以进行句子相似度的预测，句子翻译，语法判断，句子摘要等等功能。
常见的预训练数据，可以从维基百科等上找，再通过简单的选择进行一定的数据清理（得到例如C4这样的数据集），从而进行模型的预训练
如果所需要的任务本身已经具有较大数量的高质量数据集，那么预训练的作用反而会被削弱。
通过对比实验，干净的预训练数据集，确实会提升训练效果。预训练数据集来自不同的专业方向，对测试也会有较为明显的影响。训练时间对结果也会用较多影响（有必要进行一定要的重复，以完成梯度下降但不能过量重复）
训练多语言模型，（如mT5）可以采用模型生成的数据进行进一步训练。从而提供更多的数据（一种方法）。
我们希望模型可以学习到很多的知识，从而可以帮助我们更加便捷的提取（根据描述回答信息且不提供上下文）。
是否可以缩小大小模型之间的差距呢？目前看来较为困难，当模型达到足够大的大小事，出现的性能提升是爆发式的。但提升的主要还是模型对知识的记忆量，而非是提升的推理能力。

