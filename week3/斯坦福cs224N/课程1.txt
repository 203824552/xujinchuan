许多传统NLP的问题在于将单词视为离散的符号，导致没有联系起来
可以通过上下文的关系来确定一个词语的意思
将所有的单词，存放在一个高维空间中

word2vec：基于当前的词向量，根据模型计算中心词的上下文词出现的概率，通过不断地学习，使得要出现的词概率更大。一般选择根据词向量进行概率的运算，对概率进行归一化（softmax）

优秀的词向量，进行加减时可以得到较好的效果，比如国王减去男性加上女性成为女王，很多关系就因此应运而生，（国王与男人就像女王与女人）

反义词的向量其实也是相似的。

整体过程则是，先生成随机词向量，再反复迭代（根据梯度调整），得到较好的结果