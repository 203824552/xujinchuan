有限的词汇表需要进行发散，所以出现了UNK进行缓解，不然面对崭新的单词则会手足无措。
子词模型，可以将不认识的单词分解为一些子词，如a，b经常出现，则出现了新词ab。尽量少的处理，比如保留标点符号等也会提升模型的泛化性，频率出现较高的的单词就会出现在单词表中，较长的单词也会出现。
一个词语的完整意义往往与上下文相关。
预训练，可以用于对于我们想要使用的语言模型进行初始化，通过大量的数据训练可能会习得一些潜在知识逻辑。与之相对的则是微调，数量级更小，但是更关注所对应的任务。

编码器可以获得双向上下文，进行预训练时，需要进行输入的掩码设置，让其无法获得答案，从而进行预测。
BERT模型较为擅长摘要，文字填空等等

轻量级微调只选择部分参数进行修改，或者只增加一些参数用于修改，而不改变预训练模型

解码器只可以观察自身，而不能观察未来的情况，因此可以预测句子中的下一个单词，现在有很多decoder-only模型

可以通过模型的困惑度来评估难度，有事会选择很多的评估标准来证明模型的有效性，

模型大小和训练数据量往往均衡的时候效果更好。

思路链通过生成了一系列中间步骤，从而提高了正确率。（真正原因有待理解）