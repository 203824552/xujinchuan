在本次实验中，收获颇多
本次实验主要是在fastbook框架下进行NLP的学习。
首先我们需要进行的是文本的预处理，而选择的素材则是豆瓣影评
通过小小的使用案例，可以看到部分影评的文本如何
然后就是需要将文本变为机器可以识别的token（但并不是用于训练的模式），可以通过wordtokenizer方式进行构造，同样查看输出，就可以发现，文本被按照单词进行划分。

为了使文本根据有质量，我们可以采用Tokenizer进行进一步的区分，其中提供了“xxbos”表示文本开始，“xxmaj”表示文本中的大写等等。
还有别的区分方法如subwordtokenizer等等用于应对没有空格的语法
对于机器进行学习，我们还需要将token转化为index形式，在此我们采用Numericalize（）函数，并且可以通过num.vocab通过对应词表还原为token形式。

在NLP学习的过程中，我们还需要将长句子进行分段处理，分为大小相同的batch长度不够则进行补齐，与图像处理相似。通过np.array即可实现
最后通过LMDataLoader即可完成数据构造。

接下来就是准备开始学习，采用TextBlock进行文本的访问，也是进行数据的处理。然后就是进行预训练模型的选择，我们选择的是AWD_LSTM模型，与第一层读文本的层。通过模型不同层级的解冻，我们即可进行对于预训练语言模型的微调，并进行保存。

我们通过微调的模型，可以根据我们的TEXT输入，然他对于接下来的输出进行预测输出，由于训练不多，可以看到有输出，但合理性有待加强，至此我们完成了对于语言模型的微调训练。

重新设计一个DAtaBLock块进行评论积极或者消极的预测，同样接上我们的AWD_LSTM模型，再加上我们之前进行的文本读取encoder层，我们则可以进行文本读取，和评论分类，通过逐层解冻进行参数调整，并进行训练微调，我们可以发现，正确率逐步提升，达到了94的正确率。

至此实验结束，可以通过添加不同的模块对于我们需要的输出进行调整，并进行相应的训练。