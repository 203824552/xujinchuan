   自然语言处理的基本任务包括：词性标注，实体识别，共指消解，依存关系，情感分析等等。知识图谱，就是一系列结构化的知识所组合。

大模型，对于小样本学习能力较强。
word2vec：使用滑动窗口进行数据训练，最终得到预测词，对应词表的概率大小，分别输出每一个词，逐步递进，还有平衡常见词与罕见词的方法，由此来进行对于词语的训练，

RNN：不断利用上一层的数据，进行训练传递，所以可以记忆住含有顺序的内容，其中：GRU：可以控制是否让信息进入下一层，有重置门和更新门。LSTM：含有遗忘门，决定是否保存之前的状态，输入门决定输入信息，最后就是输出门，决定输出量。cnn更擅长处理局部特征。
注意力机制：
（翻译例子）则是在encoder得到n个向量，decoder再分别与之求注意力分数，得到对应的输出词汇
当向量维度不一致时，则可以使用辅助矩阵进行修改。
并且也在encoder和decoder直接提供了直接联系的方式。

transformer包括encoder和decoder，输入需要进行合理的切分，通过多个block堆叠而成，
BPE是一种新的分词方式，将单词先切分为一个个字母，不断将出现较多的频率的单词组合，进行新的组合，
位置编码：进行位置关系的区分，通过正弦函数和余弦函数区分，
Attention：主要就是q，k，v进行注意力分数的计算
transform通过mask避免模型进行后面单词的观测。适合并行运算，是预训练模型的主流运算框架，但复杂度较高

预训练模型，bert采用随机mask进行填空训练，但基础bert效率较低，所以有很多的改进工作已经出现或待发展。
可以通过多个专家模型的组合从而得到更好的模型使用效果

由于部分模型过于巨大，所以使用prompt进行微调，而不是对参数进行微调
prompt-learing：根据不同的提示就可以完成相同的任务可以通过多个prompt取平均之类的方法打到更稳定的方法
verbalizer：对词语进行分类组合，从而进行比较和选择
当模型参数量够大时，进行prompt训练才会效果良好，不然甚至会伤害模型

delta tuning：都是基于pretrain，多种方法（1）增加一些新的参数（2）选择部分参数替换（3）用低秩空间训练（在公共低维空间进行公共解的求解）并且都和全参微调相差不大。
可以通过自动搜索的方法找到较优质的解（类似消融实验）





