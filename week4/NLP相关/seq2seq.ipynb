{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk\n",
        "nltk.download()\n",
        "\n",
        "import jieba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs3cbcQeEJBA",
        "outputId": "2bc51db6-f5d4-49af-f1d3-0bf9a0a32c50"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(in_file):\n",
        "    cn = []\n",
        "    en = []\n",
        "    num_examples = 0\n",
        "    with open(in_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split('\\t')      # 每一行是英文+翻译的形式\n",
        "            #print(line)   # ['Anyone can do that.', '任何人都可以做到。']\n",
        "            #print(nltk.word_tokenize(line[0].lower()))    # ['anyone', 'can', 'do', 'that', '.']\n",
        "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])\n",
        "            #print([c for c in line[1]])   ['任', '何', '人', '都', '可', '以', '做', '到', '。']\n",
        "            #print(list(jieba.cut(line[1])))        ['任何人', '都', '可以', '做到', '。']\n",
        "            #cn.append(['BOS'] + [c for c in line[1]] + ['EOS'])\n",
        "            cn.append(['BOS'] + list(jieba.cut(line[1])) + ['EOS'])\n",
        "    return en, cn\n",
        "\n",
        "train_file = 'nmt/en-cn/train.txt'\n",
        "dev_file = 'nmt/en-cn/dev.txt'\n",
        "train_en, train_cn = load_data(train_file)\n",
        "dev_en, dev_cn = load_data(dev_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjd4AAG7GjrD",
        "outputId": "9a7ab869-c183-42e8-8a44-0a99299290ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.817 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.817 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_cn[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2uvB_S-I9J2",
        "outputId": "9ffb4af4-5b77-455b-e86a-22925d929173"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['BOS', '她', '把', '雜誌', '放在', '桌上', '。', 'EOS'], ['BOS', '嘿', '，', '你', '在', '這做', '什麼', '？', 'EOS'], ['BOS', '請', '保守', '這個', '秘密', '。', 'EOS'], ['BOS', '事情', '怎麼', '變糟', '的', '？', 'EOS'], ['BOS', '京都', '和', '波士顿', '是', '姐妹', '城市', '。', 'EOS']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_IDX = 0\n",
        "PAD_IDX = 1\n",
        "def build_dict(sentences, max_words=50000):\n",
        "    word_count = Counter()\n",
        "    for sentence in sentences:\n",
        "        for s in sentence:\n",
        "            word_count[s] += 1\n",
        "    ls = word_count.most_common(max_words)\n",
        "    total_words = len(ls) + 2    # 两个特殊的字符UNK和PAD\n",
        "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}   # 字典的前两个位置放特殊字符\n",
        "    word_dict['UNK'] = UNK_IDX\n",
        "    word_dict['PAD'] = PAD_IDX\n",
        "    return word_dict, total_words\n",
        "\n",
        "en_dict, en_total_words = build_dict(train_en)\n",
        "cn_dict, cn_total_words = build_dict(train_cn)\n",
        "\n",
        "inv_en_dict = {v:k for k, v in en_dict.items()}\n",
        "inv_cn_dict = {v:k for k, v in cn_dict.items()}"
      ],
      "metadata": {
        "id": "qvB-OEDnJGcG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
        "    length = len(en_sentences)\n",
        "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
        "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
        "    # 根据英语句子的长度排序\n",
        "    def len_argsort(seq):   # 这个seq是一个二维矩阵， 每一行是一个句子， 且都已经用单词在字典中的位置进行了编码\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "\n",
        "    if sort_by_len:\n",
        "        sorted_index = len_argsort(out_en_sentences)\n",
        "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
        "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
        "\n",
        "    return out_en_sentences, out_cn_sentences\n",
        "\n",
        "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
        "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
      ],
      "metadata": {
        "id": "yk17HaVTOZNR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_cn[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_VPmKyaOe6T",
        "outputId": "e49fe056-6c43-4eca-f5a4-63ad80df1f30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 0, 239, 4, 3], [2, 5019, 613, 3], [2, 587, 265, 4, 3], [2, 6819, 111, 4, 3], [2, 5052, 3420, 4, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minibatches(n, minibatch_size, shuffle=True):\n",
        "    idx_list = np.arange(0, n, minibatch_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx_list)\n",
        "    minibatches = []\n",
        "    for idx in idx_list:\n",
        "        minibatches.append(np.arange(idx, min(idx+minibatch_size, n)))\n",
        "    return minibatches      # 这个会返回多批连着的bath_size个索引\n",
        "#get_minibatches(len(train_en), 32)\n",
        "\n",
        "# 这个函数是在做数据预处理， 由于每个句子都不是一样长， 所以通过这个函数就可以把句子进行补齐， 不够长的在句子后面添加0\n",
        "def prepare_data(seqs):\n",
        "    lengths = [len(seq) for seq in seqs]    # 得到每个句子的长度\n",
        "    n_samples = len(seqs)       # 得到一共有多少个句子\n",
        "    max_len = np.max(lengths)              # 找出最大的句子长度\n",
        "\n",
        "    x = np.zeros((n_samples, max_len)).astype('int32')    # 按照最大句子长度生成全0矩阵\n",
        "    x_lengths = np.array(lengths).astype('int32')\n",
        "    for idx, seq in enumerate(seqs):        # 把有句子的位置填充进去\n",
        "        x[idx, :lengths[idx]] = seq\n",
        "    return x, x_lengths      # x_mask\n",
        "\n",
        "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
        "    minibatches = get_minibatches(len(en_sentences), batch_size)   # 得到batch个索引\n",
        "    all_ex = []\n",
        "    for minibatch in minibatches:   # 每批数据的索引\n",
        "        mb_en_sentences = [en_sentences[t] for t in minibatch]   # 取数据\n",
        "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]  # 取数据\n",
        "        mb_x, mb_x_len = prepare_data(mb_en_sentences) # 填充成一样的长度， 但是要记录一下句子的真实长度， 这个在后面输入网络的时候得用\n",
        "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
        "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
        "    return all_ex\n",
        "\n",
        "batch_size = 64\n",
        "train_data = gen_examples(train_en, train_cn, batch_size)   # 产生训练集\n",
        "random.shuffle(train_data)\n",
        "dev_data = gen_examples(dev_en, dev_cn, batch_size)   # 产生验证集"
      ],
      "metadata": {
        "id": "5Atl_ueHOmZ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[1][0].shape, train_data[1][1].shape, train_data[1][2].shape, train_data[1][3].shape)\n",
        "# 第一个维度表示第1个batch， 第二个维度[0]代表每个每个句子单词个数， [1]代表每个句子的长度， [2]代表中文词个数， [3]代表每个句子的中文长度\n",
        "# 注意每个batch里面的句子长度是不一样的， 同一batch里面的句子长度由于填充0使得一样了"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAkUh4MlO0vW",
        "outputId": "ce98ba18-d9b5-42a3-f0f0-38ca97d72e75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 12) (64,) (64, 15) (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "tensor_in = torch.FloatTensor([[1, 2, 3], [5, 0, 0]]).resize_(2, 3, 1)\n",
        "tensor_in = Variable(tensor_in)\n",
        "seq_lenghs = [3, 1]\n",
        "tensor_in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKf81mVMQJMz",
        "outputId": "2c5d5d94-ed4c-4671-d7c3-e906ba0ed856"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.],\n",
              "         [2.],\n",
              "         [3.]],\n",
              "\n",
              "        [[5.],\n",
              "         [0.],\n",
              "         [0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(seq_lenghs).sort(0, descending=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRru79GjRMVA",
        "outputId": "b082ffd4-c400-4ee3-ed75-6463abac8908"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.sort(\n",
              "values=tensor([3, 1]),\n",
              "indices=tensor([0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pack = nn.utils.rnn.pack_padded_sequence(tensor_in, seq_lenghs, batch_first=True)\n",
        "pack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1dwautDRVD5",
        "outputId": "e3f3c870-f36a-4287-c8ab-7ab101a4d2fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[1.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [3.]]), batch_sizes=tensor([2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.RNN(1, 2, 3, batch_first=True)   # 输入维度是1(embed_dim)， 输出维度是2(2个隐藏单元), 3层\n",
        "h0 = Variable(torch.randn(3, 2, 2))  # h0的初始状态， (layers_num*direction_nums, batch_size, hidden_size)\n",
        "\n",
        "out, h = rnn(pack, h0)\n",
        "out[0].shape   # [4, 2]\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhhU_0MGRgAD",
        "outputId": "65dfc00e-e011-4d7a-bebf-c6899057e85d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[ 0.7026, -0.1171],\n",
              "        [ 0.6928,  0.0010],\n",
              "        [ 0.4563, -0.1514],\n",
              "        [ 0.5318, -0.2126]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PlainEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
        "        super(PlainEncoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        #这里需要输入lengths, 因为每个句子是不一样长的，我们需要每个句子最后一个时间步的隐藏状态，\n",
        "        #所以需要知道句子有多长，x表示一个batch里面的句子\n",
        "\n",
        "        #把batch里面的seq按照长度排序\n",
        "        sorted_len, sorted_idx = lengths.sort(0, descending=True) #sorten表示排好序的数组，sorted_index表示每个元素再原数组位置\n",
        "        x_sorted = x[sorted_idx.long()] #句子已经按照seq长度排好序\n",
        "        embedded = self.dropout(self.embed(x_sorted)) #[batch_size, seq_len, embed_size]\n",
        "\n",
        "        #下面一段代码处理变长序列\n",
        "        #这里的data.numpy()是原始张量的克隆，然后转成了numpy数组，相当于clone().numpy()\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        #上面这句话之后，会把变长序列的0都给去掉，之前填充的字符都给压扁\n",
        "        packed_out, hid = self.rnn(packed_embedded)#通过这句话就可以得到batch中每个样本的真实隐藏状态\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)#这里是再填充回去，看下面的例子就懂了\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False) #这里是为了还是让短的句子再前面\n",
        "        out = out[original_idx.long()].contiguous() #contiguous是为了把不连续的内存单元连续起来\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "\n",
        "        return out, hid[[-1]] #把最后一层的his给拿出来，这个具体看上面的简单演示"
      ],
      "metadata": {
        "id": "KeB8k3UQTj28"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#这个基本上和Encoder是一致得，无非就是初始化得h换成了Encoder之后得h\n",
        "class PlainDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
        "        super(PlainDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, y, y_lengths, hid):\n",
        "        #y:[batch_size, seq_len-1]\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True) #依然是句子从长到短排序\n",
        "        y_sorted = y[sorted_idx.long()]\n",
        "        hid = hid[:, sorted_idx.long()]\n",
        "\n",
        "        y_sorted = self.dropout(self.embed(y_sorted)) #[batch_size, output_length, embed_size]\n",
        "\n",
        "        pack_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(pack_seq, hid) #这个计算得是每个有效时间步单词得最后一层得隐藏状态\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True) #[batch,seq_len-1, hidden_size]\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguous() #[batch, seq_len-1, hidden_size]\n",
        "\n",
        "        hid = hid[:, original_idx.long()].contiguous() #[1,batch, hidden_size]\n",
        "        output = F.log_softmax(self.out(output_seq), -1)\n",
        "        #[batch, seq_len-1, vocab_size] 表示每个样本每个时间步长都有一个vocab_size得维度长度，表示每个单词得概率\n",
        "\n",
        "        return output, hid"
      ],
      "metadata": {
        "id": "S8DQBwUGUG3T"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlainSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(PlainSeq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths) #encoder进行编码\n",
        "        output, hid = self.decoder(y, y_lengths, hid) #decoder负责解码\n",
        "        return output, None\n",
        "    def translate(self, x, x_lengths, y, max_length=10):#这个是进来一个句子进行翻译 max_length句子得最大长度\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            output, hid = self.decoder(y, torch.ones(batch_size).long().to(y.device), hid=hid)\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            preds.append(y)\n",
        "        return torch.cat(preds, 1), None"
      ],
      "metadata": {
        "id": "82VrD8H_ZiRX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# masked cross entropy loss\n",
        "class LanguageModelCriterion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageModelCriterion, self).__init__()\n",
        "\n",
        "    def forward(self, input, target, mask):\n",
        "        # input: [batch_size, seq_len, vocab_size]    每个单词的可能性\n",
        "        input = input.contiguous().view(-1, input.size(2))   # [batch_size*seq_len-1, vocab_size]\n",
        "        target = target.contiguous().view(-1, 1)    #  [batch_size*seq_len-1, 1]\n",
        "\n",
        "        mask = mask.contiguous().view(-1, 1)   # [batch_size*seq_len-1, 1]\n",
        "        output = -input.gather(1, target) * mask # 在每个vocab_size维度取正确单词的索引， 但是里面有很多是填充进去的， 所以mask去掉这些填充的\n",
        "        # 这个其实在写一个NLloss ， 也就是sortmax的取负号\n",
        "        output = torch.sum(output) / torch.sum(mask)\n",
        "\n",
        "        return output  # [batch_size*seq_len-1, 1]"
      ],
      "metadata": {
        "id": "X-5VI6RRZ0pI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dropout = 0.2\n",
        "hidden_size = 100\n",
        "encoder = PlainEncoder(vocab_size=en_total_words, hidden_size=hidden_size, dropout=dropout)\n",
        "decoder = PlainDecoder(vocab_size=cn_total_words, hidden_size=hidden_size, dropout=dropout)\n",
        "\n",
        "model = PlainSeq2Seq(encoder, decoder)\n",
        "model = model.to(device)\n",
        "loss_fn = LanguageModelCriterion().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "NKeiYBUCZm1R"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_num_words = total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()    # 这个是一个batch的英文句子 大小是[batch_size, seq_len]\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()    # 每个句子的长度\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()  # 解码器那边的输入， 输入一个单词去预测另外一个单词\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()   # 解码器那边的输出  [batch_size, seq_len-1]\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()  # 这个减去1， 因为没有了最后一个  [batch_size, seq_len-1]\n",
        "            mb_y_len[mb_y_len<=0] =  1   # 这句话是为了以防出错\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            # [batch_size, mb_y_len.max()], 上面是bool类型， 下面是float类型， 只计算每个句子的有效部分， 填充的那部分去掉\n",
        "            mb_out_mask = mb_out_mask.float()  # [batch_size, seq_len-1]  因为mb_y_len.max()就是seq_len-1\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "    print('Evaluation loss', total_loss / total_num_words)\n",
        "\n",
        "def train(model, data, num_epochs=20):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_num_words = total_loss = 0.\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
        "            mb_y_len[mb_y_len<=0] = 1\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            mb_out_mask = mb_out_mask.float()\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "\n",
        "            # 更新\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)     # 这里防止梯度爆炸， 这是和以往不太一样的地方\n",
        "            optimizer.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                print('Epoch', epoch, 'iteration', it, 'loss', loss.item())\n",
        "\n",
        "        print('Epoch', epoch, 'Training loss', total_loss / total_num_words)\n",
        "        if epoch % 5 == 0:\n",
        "            evaluate(model, dev_data)\n",
        "\n",
        "# 训练\n",
        "train(model, train_data, num_epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2r2-MTlZ7DN",
        "outputId": "fe2a10c8-bf19-49d6-a71f-8a01a32196b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 loss 9.331954956054688\n",
            "Epoch 0 iteration 100 loss 6.044123649597168\n",
            "Epoch 0 iteration 200 loss 5.020909786224365\n",
            "Epoch 0 Training loss 5.850677881439716\n",
            "Evaluation loss 5.202617210244227\n",
            "Epoch 1 iteration 0 loss 4.993743419647217\n",
            "Epoch 1 iteration 100 loss 5.517873764038086\n",
            "Epoch 1 iteration 200 loss 4.523971080780029\n",
            "Epoch 1 Training loss 4.933376639293118\n",
            "Epoch 2 iteration 0 loss 4.642943382263184\n",
            "Epoch 2 iteration 100 loss 5.179933071136475\n",
            "Epoch 2 iteration 200 loss 4.205909729003906\n",
            "Epoch 2 Training loss 4.601179413637688\n",
            "Epoch 3 iteration 0 loss 4.386545181274414\n",
            "Epoch 3 iteration 100 loss 4.938098430633545\n",
            "Epoch 3 iteration 200 loss 3.9619898796081543\n",
            "Epoch 3 Training loss 4.355408479520593\n",
            "Epoch 4 iteration 0 loss 4.157600402832031\n",
            "Epoch 4 iteration 100 loss 4.773762226104736\n",
            "Epoch 4 iteration 200 loss 3.767699956893921\n",
            "Epoch 4 Training loss 4.152598270888499\n",
            "Epoch 5 iteration 0 loss 3.964405059814453\n",
            "Epoch 5 iteration 100 loss 4.622564792633057\n",
            "Epoch 5 iteration 200 loss 3.601074457168579\n",
            "Epoch 5 Training loss 3.968730022334133\n",
            "Evaluation loss 4.499227637751899\n",
            "Epoch 6 iteration 0 loss 3.7968790531158447\n",
            "Epoch 6 iteration 100 loss 4.453012466430664\n",
            "Epoch 6 iteration 200 loss 3.456566333770752\n",
            "Epoch 6 Training loss 3.804648976646861\n",
            "Epoch 7 iteration 0 loss 3.622903347015381\n",
            "Epoch 7 iteration 100 loss 4.3155694007873535\n",
            "Epoch 7 iteration 200 loss 3.309298515319824\n",
            "Epoch 7 Training loss 3.6521828084080075\n",
            "Epoch 8 iteration 0 loss 3.472048044204712\n",
            "Epoch 8 iteration 100 loss 4.170532703399658\n",
            "Epoch 8 iteration 200 loss 3.1594181060791016\n",
            "Epoch 8 Training loss 3.513998230235213\n",
            "Epoch 9 iteration 0 loss 3.325749158859253\n",
            "Epoch 9 iteration 100 loss 4.084189414978027\n",
            "Epoch 9 iteration 200 loss 3.055406332015991\n",
            "Epoch 9 Training loss 3.38225640899554\n",
            "Epoch 10 iteration 0 loss 3.1932127475738525\n",
            "Epoch 10 iteration 100 loss 3.9615941047668457\n",
            "Epoch 10 iteration 200 loss 2.9092214107513428\n",
            "Epoch 10 Training loss 3.2586401708930057\n",
            "Evaluation loss 4.358139296176464\n",
            "Epoch 11 iteration 0 loss 3.070127248764038\n",
            "Epoch 11 iteration 100 loss 3.843540906906128\n",
            "Epoch 11 iteration 200 loss 2.8215959072113037\n",
            "Epoch 11 Training loss 3.146117301025469\n",
            "Epoch 12 iteration 0 loss 2.965655565261841\n",
            "Epoch 12 iteration 100 loss 3.762136220932007\n",
            "Epoch 12 iteration 200 loss 2.701772451400757\n",
            "Epoch 12 Training loss 3.0351552764132657\n",
            "Epoch 13 iteration 0 loss 2.870527982711792\n",
            "Epoch 13 iteration 100 loss 3.6501903533935547\n",
            "Epoch 13 iteration 200 loss 2.660695791244507\n",
            "Epoch 13 Training loss 2.933677269421918\n",
            "Epoch 14 iteration 0 loss 2.7440261840820312\n",
            "Epoch 14 iteration 100 loss 3.57405686378479\n",
            "Epoch 14 iteration 200 loss 2.587522029876709\n",
            "Epoch 14 Training loss 2.838481538825214\n",
            "Epoch 15 iteration 0 loss 2.677828550338745\n",
            "Epoch 15 iteration 100 loss 3.4726216793060303\n",
            "Epoch 15 iteration 200 loss 2.467982769012451\n",
            "Epoch 15 Training loss 2.7484231285602734\n",
            "Evaluation loss 4.33167060595248\n",
            "Epoch 16 iteration 0 loss 2.536900281906128\n",
            "Epoch 16 iteration 100 loss 3.3671536445617676\n",
            "Epoch 16 iteration 200 loss 2.3839833736419678\n",
            "Epoch 16 Training loss 2.6645616664932614\n",
            "Epoch 17 iteration 0 loss 2.472564458847046\n",
            "Epoch 17 iteration 100 loss 3.297027349472046\n",
            "Epoch 17 iteration 200 loss 2.293379545211792\n",
            "Epoch 17 Training loss 2.581025452165822\n",
            "Epoch 18 iteration 0 loss 2.3635568618774414\n",
            "Epoch 18 iteration 100 loss 3.209679365158081\n",
            "Epoch 18 iteration 200 loss 2.253490924835205\n",
            "Epoch 18 Training loss 2.5067123100058937\n",
            "Epoch 19 iteration 0 loss 2.262746572494507\n",
            "Epoch 19 iteration 100 loss 3.115025281906128\n",
            "Epoch 19 iteration 200 loss 2.174926280975342\n",
            "Epoch 19 Training loss 2.4379885889968405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention model"
      ],
      "metadata": {
        "id": "Oe54zCRrhLXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(enc_hidden_size*2, dec_hidden_size)\n",
        "    def forward(self, x, lengths):\n",
        "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
        "        x_sorted = x[sorted_idx.long()]\n",
        "        embedded = self.dropout(self.embed(x_sorted)) #[batch_size, seq_len, embed_size]\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        packed_out, hid = self.rnn(packed_embedded)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)#[batch_size,seq_len,2*enc_hidden_size]\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        out = out[original_idx.long()].contiguous() #[batch_size, seq_len, 2*enc_hidden_size]\n",
        "        hid = hid[:, original_idx.long()].contiguous() #[2,batch_size, enc_hidden_size]\n",
        "\n",
        "        hid = torch.cat([hid[-2], hid[-1]], dim=1) #双向的GRU，这里是最后一个状态，联结起来 [batch_size, 2*enc_hidden_size]\n",
        "        hid = torch.tanh(self.fc(hid)).unsqueeze(0) #[1, batch_size, dec_hidden_size]\n",
        "\n",
        "        return out, hid"
      ],
      "metadata": {
        "id": "YQLiKQN0hOQf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.enc_hidden_size = enc_hidden_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "\n",
        "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
        "        self.linear_out = nn.Linear(enc_hidden_size*2+dec_hidden_size, dec_hidden_size)\n",
        "\n",
        "    def forward(self, output, encoder_output, mask):\n",
        "        # output: [batch_size, seq_len_y-1, dec_hidden_size]  这个output 是decoder的每个时间步输出的隐藏状态\n",
        "        # encoder_output: [batch_size, seq_len_x, 2*enc_hidden_size]\n",
        "        batch_size = output.size(0)\n",
        "        output_len = output.size(1)\n",
        "        input_len = encoder_output.size(1)\n",
        "\n",
        "        context_in = self.linear_in(encoder_output.view(batch_size*input_len, -1))  # [batch_size*seq_len_x,dec_hidden_size]\n",
        "        context_in = context_in.view(batch_size, input_len, -1)  # [batch_size, seq_len_x, dec_hidden_size]\n",
        "        context_in = context_in.transpose(1, 2)   # [batch_size, dec_hidden_size, seq_len_x]\n",
        "\n",
        "        attn = torch.bmm(output, context_in)  # [batch_size, seq_len_y-1, seq_len_x]\n",
        "        # 这个东西就是求得当前时间步的输出output和所有输入相似性关系的一个得分score , 下面就是通过softmax把这个得分转成权重\n",
        "        attn = F.softmax(attn, dim=2)    # 此时第二维度的数字全都变成了0-1之间的数， 越大表示当前的输出output与哪个相关程度越大\n",
        "\n",
        "        context = torch.bmm(attn, encoder_output)   # [batch_size, seq_len_y-1, 2*enc_hidden_size]\n",
        "\n",
        "        output = torch.cat((context, output), dim=2)  # [batch_size, seq_len_y-1, 2*enc_hidden_size+dec_hidden_size]\n",
        "\n",
        "        output = output.view(batch_size*output_len, -1)   # [batch_size*seq_len_y-1, 2*enc_hidden_size+dec_hidden_size]\n",
        "        output = torch.tanh(self.linear_out(output))     # [batch_size*seq_len_y-1, dec_hidden_size]\n",
        "        output = output.view(batch_size, output_len, -1)  # [batch_size, seq_len_y-1, dec_hidden_size]\n",
        "\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "ZLeK-r6RhQtV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def create_mask(self, x_len, y_len):\n",
        "        # a mask of shape x_len*y_len\n",
        "        x_mask = torch.arange(x_len.max(), device=x_len.device)[None, :] < x_len[:, None]\n",
        "        y_mask = torch.arange(y_len.max(), device=x_len.device)[None, :] < y_len[:, None]\n",
        "\n",
        "        x_mask = x_mask.float()\n",
        "        y_mask = y_mask.float()\n",
        "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, encoder_out, encoder_out_lengths, y, y_lengths, hid):\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
        "        y_sorted = y[sorted_idx.long()]   # 句子从长到短排序\n",
        "        hid = hid[:, sorted_idx.long()]\n",
        "\n",
        "        y_sorted = self.dropout(self.embed(y_sorted))     # [batch_size, output_length, embed_size]\n",
        "\n",
        "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(packed_seq, hid)\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguous()   # [batch_size, seq_len_y-1, dec_hidden_size]\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "\n",
        "        mask = self.create_mask(y_lengths, encoder_out_lengths)\n",
        "\n",
        "        output, attn = self.attention(output_seq, encoder_out, mask)\n",
        "        output = F.log_softmax(self.out(output), -1)\n",
        "\n",
        "        return output, hid, attn"
      ],
      "metadata": {
        "id": "r64BEBKnhSlC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        output, hid, attn = self.decoder(encoder_out, x_lengths, y, y_lengths, hid)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "    def translate(self, x, x_lengths, y, max_length=100):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            output, hid, attn = self.decoder(encoder_out, x_lengths, y, torch.ones(batch_size).long().to(y.device), hid)\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            preds.append(y)\n",
        "            attns.append(attn)\n",
        "\n",
        "        return torch.cat(preds, 1), torch.cat(attns, 1)"
      ],
      "metadata": {
        "id": "K_cETiCkhVYn"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}