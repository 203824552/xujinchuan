在该次实验中，我们的目标是进行自己的NLP模型的构建。
由于这次主要目的在于熟悉模型结构，因此我们选择的的数据集较为简单是一个英文从数字1写道数字10000的数据集，同样我们使用tokens和nums进行token化和index化
第一版的模型则是只是用了词嵌入层，隐藏层和输出层，前向传递过程中每次激活，每输入三个词语进行一次预测，最终达到了49的正确率。
第二版的模型则是采用循环的写法，简化了前向传播代码构造，更为灵活。正确率还是48左右。
第三版模型为了避免保存了过多的历史梯度，因此采用了detach功能，只保留最新的三个梯度，最终正确率达到了59
第四版模型为了更高效的利用输出，我们每一次都采取输入一个即预测下一个单词，故此正确率提升到了60.
第五版模型我们想要将隐藏层与另一个RNN相连接，从而更好地激活，不过效果并不是很明显，可能是因为出现了梯度爆炸或者梯度消失等问题，这也是RNN常见的问题。

因此我们进行了第六版实验，采用LSTM格式，通过cell-gate可以较好的保存较远位置的数据集的知识，从而更好的学习，一共有遗忘门，选择对应的知识忘记，输入门和细胞们选择对应的激活位置，然后输出门选择合适的输出，这就是LSTM的基本构造正确率也来到了77.
由于过拟合问题的存在，对于文本数据我们不能进行随意的文本剪裁，但我们可以使用dropout或者正则化等方式进行模型的增强，以避免过拟合，基于此设计了第七版模型，最终也是达到了83的正确率。

这次实验中，整体模型深度不深，训练较快，结果易于观察，提升了我们对NLP基本模型结构的认知，提升了基础水平。