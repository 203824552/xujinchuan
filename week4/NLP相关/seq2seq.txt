本实验旨在通过进行一个简单的翻译模型的设计，让我们更熟悉pytorch的使用。
首先需要进行的是对文本的token处理，成功将其分离成一个个单词，或词语。然后根据词语出现的频率进行字典的保存，并依据字典进行token的index化，最后再进行训练所需的batch生成，即可完成数据集的处理。
简单的的seq2seq即由一个encoder和一个decoder组成，encoder通过对我们的输入的语言句子进行编码，并得到一个最终的隐藏层状态h，decoder则通过输入“开始”字符后进行预测输出，其输入还有encoder提供的状态h。其中需要注意我们为了保持句子等长从而做的填充工作，需要在模型训练时注意还原，否则短句子会受到较大的影响。该模型中decoder就是与RNN相似，不断结合前文进行推理。
训练则是同往常一样，通过对于每一个batch，通过设计的适合于NLP的loss值进行训练，反向传递，最终即可完成我们的翻译模型。

