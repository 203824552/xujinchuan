QA，问答任务，如今的问答系统，几乎都建立在预训练模型之上。阅读理解也是问答系统的一个重要问题。
现在很多问答模型都是基于BERT。将问题作为片段A，文章作为片段B，在B中找到起始和终结点即可完成阅读理解。
可以通过注意力机制，寻找问题与哪些上下文词汇最相关，也可以通过上下文的关系来反馈查询。双向LSTM对学习效果提升较为明显的。
可以通过改变预训练方式（BERT）如将随机mask改为以片段为单位mask是可以提升问答训练的性能的。
由于是根据相似度进行计算，所以阅读理解时，加入对抗样本，如只改变一点关键词，则会对模型造成较为明显的影响，在部分未知邻域效果也较差。
开放域问答则是，根据海量数据中，找到相应的知识，（对比只处理特定领域的问题）。一种工作模式则是，先选择部分文章，再根据选中的少量文章进行总结以得出答案。
闭环问答系统，则是不依赖文本，模型可以自己进行输出。