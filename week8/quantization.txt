模型量化是一种降低深度学习模型大小和加速其推理速度的技术。它通过减少模型中参数的比特数来实现这一目的，通常将32位浮点数（FP32）量化为更低的位数值，如16位浮点数（FP16）、8位整数（INT8）等。
可以有如下优势：
减少内存使用：更小的模型占用更少的内存，使部署在资源受限的设备上成为可能。
加速推理：量化模型可以在支持硬件上实现更快的推理速度。
降低能耗：减小模型大小和提高推理速度可以降低运行时的能耗。
pytorch本身提供一些模型量化的方法，支持混合精度训练，本次就通过pytorch的提供进行了一些简单的量化测试。