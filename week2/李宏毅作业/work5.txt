本次实验是生成一个翻译模型，将英语进行翻译

选择TED2020作为中英对应的语料，并进行相应的预处理，包括全角字符转变为半角字符，返回文本长度

由于可能出现词表中不存在的单词，我们可以通过使用字词作为基本单位进行缓解，同时我们需要使用数据二值化，进行目标语言和源语言的配对

依旧是设置dropout 学习率 训练次数等基本训练参数，同时我们也需要设计数据迭代器：
控制每个 batch 不超过 N 个 token，这样可以优化 GPU 内存效率
在每个 epoch 都对训练集进行随机打乱
忽略超过最大长度的句子
将一个 batch 中的所有句子填充到相同的长度，这样可以利用 GPU 进行并行计算
添加 eos 并移动一个 token

对于NLP模型需要进行encoder，编码器顺序地读入输入序列，并且在每一个时间步输出一个单独的向量，然后在最后一个时间步输出最终的隐藏状态，或者称为内容向量（content vector）。通过使用含有注意力机制的decoder，解码器会根据当前时间步的输入（前一时间步的输出）改变其隐藏状态，并生成一个输出

模型的主要结构就是：
由编码器和解码器组成
接收输入并传递给编码器
将编码器的输出传递给解码器
解码器会根据前一时间步的输出以及编码器的输出进行解码
解码完成后，返回解码器的输出

对于损失我们则采用Label Smoothing Regularization：
让模型学习生成更少集中的分布，防止过度自信
有时候正确答案可能不是唯一的。因此，在计算损失时，我们为错误标签保留一些概率。
避免过拟合

该作业比较复杂，部分细节还有待继续了解，如何进行结构部分的调整还不是非常明晰